{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Number of images found:  30000\n",
      "Number of labels found:  30000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "class Food101Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for handling the Food-101 dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, num_samples_per_split=30000):\n",
    "        \"\"\"\n",
    "        Initialize the Food-101 dataset.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Path to the root directory of the Food-101 dataset.\n",
    "            num_samples_per_split (int): Number of samples to load for each split (training and validation).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.num_samples_per_split = num_samples_per_split\n",
    "\n",
    "        # Create a list of image file paths and their corresponding labels\n",
    "        self.image_paths, self.labels = self._load_data()\n",
    "\n",
    "        print('Number of images found: ', len(self.image_paths))\n",
    "        print('Number of labels found: ', len(self.labels))\n",
    "\n",
    "        self.transform = Compose([\n",
    "            Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        category_name_to_index = {}\n",
    "        for index, category in enumerate(os.listdir(os.path.join(self.data_dir, 'images'))):\n",
    "            category_name_to_index[category] = index\n",
    "\n",
    "        self.category_name_to_index = category_name_to_index\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Load image file paths and labels from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list, list: A list of image file paths and a list of corresponding labels.\n",
    "        \"\"\"\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "\n",
    "        # only include images that are in the training set\n",
    "        # I can tell if an image is in the training set by looking at the file name\n",
    "        # and seeing if it is in /meta/train.txt\n",
    "\n",
    "        # Load the train.txt file\n",
    "        with open(os.path.join(self.data_dir, 'meta/train.txt'), 'r') as f:\n",
    "            train_files = f.readlines()\n",
    "\n",
    "        # Extract the image file name from each line\n",
    "        train_files = [file.split('/')[1].strip() + '.jpg' for file in train_files]\n",
    "\n",
    "        # Traverse through each food category\n",
    "        for category in os.listdir(os.path.join(self.data_dir, 'images')):\n",
    "            category_path = os.path.join(self.data_dir, 'images', category)\n",
    "\n",
    "            # Ensure it's a directory\n",
    "            if os.path.isdir(category_path):\n",
    "                for image_filename in os.listdir(category_path):\n",
    "                    if image_filename.endswith('.jpg') and image_filename in train_files:\n",
    "                        image_path = os.path.join(category_path, image_filename)\n",
    "                        image_paths.append(image_path)\n",
    "                        labels.append(category)\n",
    "\n",
    "                    # Stop loading once we reach the desired number of samples per split\n",
    "                    if len(image_paths) >= self.num_samples_per_split:\n",
    "                        break\n",
    "\n",
    "                # Additional check to ensure we have exactly the desired number of samples\n",
    "                if len(image_paths) >= self.num_samples_per_split:\n",
    "                    break\n",
    "\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transform here\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        category_index = torch.tensor(self.category_name_to_index[label])\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': category_index,\n",
    "            'filepath': image_path,\n",
    "        }\n",
    "\n",
    "    def get_splits(self):\n",
    "        \"\"\"\n",
    "        Generate training and validation indices for training.\n",
    "        \"\"\"\n",
    "        # Create a list of indices for the dataset split\n",
    "        indices = list(range(len(self.image_paths)))\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create the train and validation subsets\n",
    "        train_subset = Subset(self, train_indices)\n",
    "        val_subset = Subset(self, val_indices)\n",
    "\n",
    "        return train_subset, val_subset\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Split the input into self.num_heads different heads\n",
    "        query, key, value = self.query(x), self.key(x), self.value(x)\n",
    "        \n",
    "        query = query.view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        key = key.view(B, N, self.num_heads, self.head_dim).permute(0, 2, 3, 1)\n",
    "        value = value.view(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Calculate scaled dot-product attention\n",
    "        scores = torch.matmul(query, key) * self.scale\n",
    "        attention = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        \n",
    "        x = torch.matmul(attention, value)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(B, N, C)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention_heads = nn.ModuleList([Attention(embed_dim, num_heads) for _ in range(num_heads)])\n",
    "        self.fc_out = nn.Linear(embed_dim * num_heads, embed_dim)  # Adjust in_features and out_features\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_outputs = [head(x) for head in self.attention_heads]\n",
    "        concat_attention = torch.cat(attention_outputs, dim=2)\n",
    "        out = self.fc_out(concat_attention)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, num_patches, embed_dim, num_heads, num_layers, dropout_rate=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = int(math.sqrt(224 * 224 / num_patches))  # Correctly calculate the patch size\n",
    "        self.num_patches_h = int(224 / self.patch_size)\n",
    "        self.num_patches_w = int(224 / self.patch_size)\n",
    "        self.patch_embedding = nn.Linear(3 * self.patch_size * self.patch_size, embed_dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.transformer_layers = nn.ModuleList([MultiHeadAttention(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  # Get batch size, channels, height, and width\n",
    "        x = x.permute(0, 2, 3, 1)  # Permute to (Batch Size, Height, Width, Channels)\n",
    "        \n",
    "        # Calculate the size of each patch\n",
    "        self.num_patches_h = int(H / self.patch_size)\n",
    "        self.num_patches_w = int(W / self.patch_size)\n",
    "        \n",
    "        # Split the image into patches\n",
    "        x = x.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n",
    "        \n",
    "        # Reshape to (Batch Size, num_patches_h * num_patches_w, 3 * self.patch_size * self.patch_size)\n",
    "        x = x.contiguous().view(B, self.num_patches, 3 * self.patch_size * self.patch_size)\n",
    "        \n",
    "        x = self.patch_embedding(x)\n",
    "        x = x + self.positional_embedding\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "class Config:\n",
    "    num_classes = 101\n",
    "    num_patches = 256\n",
    "    \n",
    "    embed_dim = 64\n",
    "    num_heads = 8\n",
    "    num_layers = 12\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 64\n",
    "    batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_handler = Food101Dataset(data_dir='food101', num_samples_per_split=30000)\n",
    "\n",
    "train_data, val_data = data_handler.get_splits()\n",
    "\n",
    "# Create data loaders using the batch_size from the Config class\n",
    "train_loader = DataLoader(train_data, batch_size=Config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=Config.batch_size, shuffle=True)\n",
    "\n",
    "model = VisionTransformer(Config.num_classes, Config.num_patches, Config.embed_dim, Config.num_heads, Config.num_layers)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.7960: 100%|██████████| 750/750 [04:27<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/64] - Loss: 3.7833 - Validation Loss: 3.7072 - Validation Accuracy: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.6887: 100%|██████████| 750/750 [04:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/64] - Loss: 3.7037 - Validation Loss: 3.6970 - Validation Accuracy: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.4776: 100%|██████████| 750/750 [04:21<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/64] - Loss: 3.6545 - Validation Loss: 3.5710 - Validation Accuracy: 0.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.5551: 100%|██████████| 750/750 [04:21<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/64] - Loss: 3.5451 - Validation Loss: 3.5213 - Validation Accuracy: 0.0588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.6010: 100%|██████████| 750/750 [04:23<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/64] - Loss: 3.5094 - Validation Loss: 3.5148 - Validation Accuracy: 0.0612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.4136: 100%|██████████| 750/750 [04:15<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/64] - Loss: 3.4913 - Validation Loss: 3.5072 - Validation Accuracy: 0.0615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.7139: 100%|██████████| 750/750 [04:21<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/64] - Loss: 3.4750 - Validation Loss: 3.4975 - Validation Accuracy: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.5118: 100%|██████████| 750/750 [04:27<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/64] - Loss: 3.4594 - Validation Loss: 3.4972 - Validation Accuracy: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.4047: 100%|██████████| 750/750 [04:24<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/64] - Loss: 3.4475 - Validation Loss: 3.4940 - Validation Accuracy: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.5766: 100%|██████████| 750/750 [04:26<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/64] - Loss: 3.4302 - Validation Loss: 3.4742 - Validation Accuracy: 0.0643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.0398: 100%|██████████| 750/750 [04:30<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/64] - Loss: 3.4152 - Validation Loss: 3.4901 - Validation Accuracy: 0.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.4034: 100%|██████████| 750/750 [04:10<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/64] - Loss: 3.4021 - Validation Loss: 3.4747 - Validation Accuracy: 0.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.0986: 100%|██████████| 750/750 [04:22<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/64] - Loss: 3.3866 - Validation Loss: 3.4780 - Validation Accuracy: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.3295: 100%|██████████| 750/750 [04:20<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/64] - Loss: 3.3694 - Validation Loss: 3.4724 - Validation Accuracy: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.2817: 100%|██████████| 750/750 [04:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/64] - Loss: 3.3574 - Validation Loss: 3.4708 - Validation Accuracy: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.1178: 100%|██████████| 750/750 [04:23<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/64] - Loss: 3.3380 - Validation Loss: 3.4896 - Validation Accuracy: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.1708: 100%|██████████| 750/750 [04:27<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/64] - Loss: 3.3239 - Validation Loss: 3.4599 - Validation Accuracy: 0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.3941: 100%|██████████| 750/750 [04:32<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/64] - Loss: 3.3000 - Validation Loss: 3.5253 - Validation Accuracy: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.2711: 100%|██████████| 750/750 [04:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/64] - Loss: 3.2856 - Validation Loss: 3.5119 - Validation Accuracy: 0.0640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.1214: 100%|██████████| 750/750 [04:29<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/64] - Loss: 3.2689 - Validation Loss: 3.5867 - Validation Accuracy: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.5373: 100%|██████████| 750/750 [04:07<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/64] - Loss: 3.2515 - Validation Loss: 3.5513 - Validation Accuracy: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.2038: 100%|██████████| 750/750 [04:14<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/64] - Loss: 3.2370 - Validation Loss: 3.6880 - Validation Accuracy: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.4266: 100%|██████████| 750/750 [04:10<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/64] - Loss: 3.2211 - Validation Loss: 3.5876 - Validation Accuracy: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.3423: 100%|██████████| 750/750 [04:04<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/64] - Loss: 3.1971 - Validation Loss: 3.6077 - Validation Accuracy: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.2591: 100%|██████████| 750/750 [03:47<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/64] - Loss: 3.1806 - Validation Loss: 3.6435 - Validation Accuracy: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.2349: 100%|██████████| 750/750 [04:11<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/64] - Loss: 3.1632 - Validation Loss: 3.7026 - Validation Accuracy: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.0934: 100%|██████████| 750/750 [04:22<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/64] - Loss: 3.1429 - Validation Loss: 3.6603 - Validation Accuracy: 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.1862: 100%|██████████| 750/750 [04:18<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/64] - Loss: 3.1213 - Validation Loss: 3.6617 - Validation Accuracy: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.3000: 100%|██████████| 750/750 [04:21<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/64] - Loss: 3.0983 - Validation Loss: 3.8713 - Validation Accuracy: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 2.9649: 100%|██████████| 750/750 [04:19<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/64] - Loss: 3.0881 - Validation Loss: 3.9037 - Validation Accuracy: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 3.1412: 100%|██████████| 750/750 [04:23<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/64] - Loss: 3.0627 - Validation Loss: 3.8495 - Validation Accuracy: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 749/750, Loss: 2.8567: 100%|██████████| 750/750 [04:20<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/64] - Loss: 3.0382 - Validation Loss: 4.0171 - Validation Accuracy: 0.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 556/750, Loss: 3.1601:  74%|███████▍  | 557/750 [03:01<01:02,  3.11it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(Config.num_epochs):\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        # Retrieve features and labels from the current batch\n",
    "        features = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation and optimization with gradient clipping\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_description(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            features = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_total_loss += val_loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "\n",
    "    average_val_loss = val_total_loss / len(val_loader)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f'Epoch [{epoch + 1}/{Config.num_epochs}] - Loss: {average_loss:.4f} - Validation Loss: {average_val_loss:.4f} - Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  10000\n",
      "Number of labels found:  10000\n",
      "Test Loss: 9.3660 - Test Accuracy: 0.0410\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test_data(model, criterion):\n",
    "    # test data can be identified in /food101/meta/test.txt\n",
    "    \n",
    "    # Load the test.txt file\n",
    "    with open(os.path.join('food101', 'meta', 'test.txt'), 'r') as f:\n",
    "        test_files = f.readlines()\n",
    "\n",
    "    # Extract the image file name from each line\n",
    "    test_files = [file.split('/')[1].strip() + '.jpg' for file in test_files]\n",
    "\n",
    "    # Create a list of image file paths and their corresponding labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Traverse through each food category\n",
    "    for category in os.listdir(os.path.join('food101', 'images')):\n",
    "        category_path = os.path.join('food101', 'images', category)\n",
    "\n",
    "        # Ensure it's a directory\n",
    "        if os.path.isdir(category_path):\n",
    "            for image_filename in os.listdir(category_path):\n",
    "                if image_filename.endswith('.jpg') and image_filename in test_files:\n",
    "                    image_path = os.path.join(category_path, image_filename)\n",
    "                    image_paths.append(image_path)\n",
    "                    labels.append(category)\n",
    "\n",
    "    # process the test data the same way I did the training data\n",
    "    test_data = Food101Dataset(data_dir='food101', num_samples_per_split=10000)\n",
    "\n",
    "    # overwrite the image_paths and labels with the test data\n",
    "    test_data.image_paths = image_paths\n",
    "    test_data.labels = labels\n",
    "\n",
    "    test_loader = DataLoader(test_data, batch_size=Config.batch_size, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f'Test Loss: {average_loss:.4f} - Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "evaluate_on_test_data(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
